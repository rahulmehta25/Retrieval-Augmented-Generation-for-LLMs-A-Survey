text_splitter:
  type: fixed_size
  chunk_size: 500
  chunk_overlap: 50

embedder:
  type: sentence_transformer
  model_name: all-MiniLM-L6-v2
  cache_dir: ./embedding_cache

vector_store:
  type: chromadb
  path: ./chroma_db
  collection_name: rag_collection
  # For FAISS, you might need:
  # type: faiss
  # embedding_dimension: 384 # For all-MiniLM-L6-v2

generator:
  type: ollama
  model_name: gemma:2b  # Lightweight 2B model - only ~1.7GB RAM required!
  # You also have installed:
  # model_name: qwen2.5-coder:7b  # Great for code-related tasks
  # Other Ollama models you can install:
  # model_name: llama2        # Meta's Llama 2 (3.8GB)
  # model_name: mistral       # Mistral 7B (4.1GB) 
  # model_name: phi           # Microsoft Phi-2 (2.7GB)
  # For OpenAI, you might need:
  # type: openai
  # model_name: gpt-3.5-turbo
  # For Ollama (local models via HTTP API), use:
  # type: ollama
  # model_name: gemma:2b        # Lightweight model, good performance
  # model_name: gemma:7b        # Better quality, more resource intensive
  # model_name: llama2:7b       # Meta's Llama 2
  # model_name: llama2:13b      # Larger Llama 2 model
  # model_name: mistral:7b      # Mistral 7B model
  # model_name: codellama:7b    # Code-focused model
  # host: localhost             # Ollama server host (default: localhost)
  # port: 11434                 # Ollama server port (default: 11434)

# Advanced features for modular architecture
reranker:
  type: cross_encoder
  model_name: cross-encoder/ms-marco-MiniLM-L-6-v2

context_compression:
  type: extractive
  max_tokens: 1000

hybrid_search:
  type: hybrid
  alpha: 0.5  # Weight for dense vs sparse search

# System configuration
max_context_tokens: 1000
conversation_memory:
  max_turns: 10
  max_tokens: 2000 